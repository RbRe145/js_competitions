{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haoruilee/js_competitions/blob/kaggle-upload-1/js-transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PDbN_7P4JYXa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import os\n",
        "import statistics as stat\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk4yq6QR2Jpu",
        "outputId": "ae98edcf-d5fa-4759-d6a0-3132336b2b67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/jane-street-real-time-market-data-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlhO24Ud2Zre",
        "outputId": "2066f533-f7ef-4c68-e324-8ae994bbcbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features.csv\t   lags.parquet    sample_submission.csv  train.parquet\n",
            "kaggle_evaluation  responders.csv  test.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./drive/jane-street-real-time-market-data-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKrrMIJF2LaH",
        "outputId": "f1e43767-3996-4eab-a848-8b6a419ff35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './drive/jane-street-real-time-market-data-forecasting': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0eCLNsNZJYXc"
      },
      "outputs": [],
      "source": [
        "\n",
        "jane_street_real_time_market_data_forecasting_path = \"./drive/MyDrive/jane-street-real-time-market-data-forecasting\"  # Set your path here\n",
        "checkpoint_dir = \"./drive/MyDrive/kaggle_js_checkpoints_transformer\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFXUfE_iJYXd",
        "outputId": "6729bd6e-8b8d-45e2-a1dc-54d576a30f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'if' statement on line 165 (<ipython-input-6-0b1b5bd2d077>, line 166)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-0b1b5bd2d077>\"\u001b[0;36m, line \u001b[0;32m166\u001b[0m\n\u001b[0;31m    print(f\"Batch {batch_idx + 1} completed in {time.time() - batch_start_time:.2f} seconds, Loss: {loss.item():.4f}\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 165\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import polars as pl\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "valid_from = 0\n",
        "\n",
        "# Load data\n",
        "start_time = time.time()\n",
        "print(\"Loading data...\")\n",
        "alltraindata = pl.scan_parquet(f\"{jane_street_real_time_market_data_forecasting_path}/train.parquet\")\n",
        "train = alltraindata.filter(pl.col(\"date_id\") >= valid_from).collect()\n",
        "print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Prepare features\n",
        "start_time = time.time()\n",
        "print(\"Preparing features...\")\n",
        "feature_names = [f\"feature_{i:02d}\" for i in range(79)]\n",
        "train_features = train.select(feature_names)\n",
        "train_features = train_features.fill_null(strategy='forward').fill_null(0)\n",
        "print(f\"Features prepared in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Prepare data\n",
        "start_time = time.time()\n",
        "print(\"Preparing data...\")\n",
        "X = train_features.to_numpy()\n",
        "y = train.select([col for col in train.columns if col.startswith('responder_')]).to_numpy()  # Use all responder columns\n",
        "print(f\"Data prepared in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Define sequence length\n",
        "seq_len = 5\n",
        "\n",
        "# Custom Dataset Class for Lazy Sequence Generation\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y, seq_len):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get sequence of features\n",
        "        X_seq = self.X[idx:idx + self.seq_len]\n",
        "        # Get the target for the end of the sequence\n",
        "        y_seq = self.y[idx + self.seq_len]\n",
        "        # Convert to tensor\n",
        "        X_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y_seq, dtype=torch.float32)\n",
        "        return X_tensor, y_tensor\n",
        "\n",
        "# Create dataset and data loader\n",
        "print('Create dataset and data loader')\n",
        "dataset = TimeSeriesDataset(X, y, seq_len=seq_len)\n",
        "batch_size = 64\n",
        "start_time = time.time()\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "print(f\"Dataset and DataLoader created in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Set up device for multi-GPU environment\n",
        "print('Set up device for multi-GPU environment')\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "        device = torch.device(\"cuda\")  # Use all available GPUs\n",
        "    else:\n",
        "        device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Warning: CUDA is not available. Training will run on CPU.\")\n",
        "\n",
        "# Transformer Model Definition\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, d_model=64, nhead=2, num_layers=2, dropout=0.2, output_size=6):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.input_projection = nn.Linear(input_size, d_model)\n",
        "        self.pos_encoder = nn.Sequential(\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.output_layer = nn.Linear(d_model, output_size)  # Output matches number of responder columns\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.output_layer(x[:, -1, :])  # Use the output from the last time step\n",
        "        return x.squeeze()\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = X.shape[1]  # Number of features per time step\n",
        "output_size = y.shape[1]  # Number of responder columns\n",
        "print('Create model')\n",
        "start_time = time.time()\n",
        "model = TransformerModel(input_size=input_size, d_model=64, nhead=2, num_layers=2, dropout=0.2, output_size=output_size)\n",
        "print(f\"Model created in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "print('Move model to GPU')\n",
        "# Set up DataParallel to use multiple GPUs\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.MSELoss()  # Base loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Custom weighted loss function to prioritize responder_6\n",
        "def weighted_loss(outputs, targets):\n",
        "    loss = F.mse_loss(outputs, targets, reduction='none')\n",
        "\n",
        "    # Find the index of 'responder_6' dynamically\n",
        "    responder_columns = [col for col in train.columns if col.startswith('responder_')]\n",
        "    responder_6_index = responder_columns.index('responder_6')\n",
        "\n",
        "    # Apply a higher weight for the 'responder_6' target column\n",
        "    weights = torch.ones_like(targets).to(device)  # Ensure weights are on GPU\n",
        "    responder_6_weight = 5.0  # Assign higher weight to responder_6\n",
        "    weights[:, responder_6_index] = responder_6_weight\n",
        "\n",
        "    weighted_loss = loss * weights\n",
        "    return weighted_loss.mean()\n",
        "\n",
        "checkpoint_dir = \"checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"Start Epoch {epoch+1}/{epochs}\")\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch_idx, (batch_X, batch_y) in enumerate(data_loader):\n",
        "        batch_start_time = time.time()\n",
        "        # Move batch data to the appropriate device\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "\n",
        "        # Compute weighted loss\n",
        "        loss = weighted_loss(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 3000 == 0:\n",
        "          print(f\"Batch {batch_idx + 1} completed in {time.time() - batch_start_time:.2f} seconds, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Print epoch timing\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] completed in {time.time() - epoch_start_time:.2f} seconds, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Save checkpoint after every epoch\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"transformer_checkpoint_epoch_{epoch+1}.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Checkpoint saved as {checkpoint_path}\")\n",
        "\n",
        "# Save the final trained model\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")\n",
        "print(\"Model saved as transformer_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load checkpoint for the model\n",
        "checkpoint_path = \"checkpoints/transformer_checkpoint_epoch_14.pth\"\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Early Stopping Parameters\n",
        "early_stopping_patience = 10\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# Track Loss for Plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop with gradient clipping and validation\n",
        "for epoch in range(15, epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"Start Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (batch_X, batch_y) in enumerate(data_loader):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(batch_X)\n",
        "        loss = weighted_loss(outputs, batch_y)\n",
        "\n",
        "        # Skip NaN losses\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"Skipping batch {batch_idx} due to NaN loss\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 3000 == 0:\n",
        "            print(f\"Batch {batch_idx + 1} completed, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    train_loss = running_loss / len(data_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_batch_X, val_batch_y in val_loader:  # Assuming val_loader is defined\n",
        "            val_batch_X, val_batch_y = val_batch_X.to(device), val_batch_y.to(device)\n",
        "            val_outputs = model(val_batch_X)\n",
        "            val_loss += weighted_loss(val_outputs, val_batch_y).item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] completed in {time.time() - epoch_start_time:.2f} seconds\")\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Update Scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
        "        print(\"Best model saved as best_transformer_model.pth\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Save checkpoint after each epoch\n",
        "    torch.save(model.state_dict(), f\"checkpoints/transformer_checkpoint_restart_epoch_{epoch+1}.pth\")\n",
        "    print(f\"Checkpoint saved as checkpoints/transformer_checkpoint_restart_epoch_{epoch+1}.pth\")\n",
        "\n",
        "# Plot Losses\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Lm5UuZZJzkCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6sTLoR2JYXg"
      },
      "outputs": [],
      "source": [
        "if run_type == 'Transformer':\n",
        "    # 模型参数\n",
        "    input_size = 79\n",
        "    d_model = 512\n",
        "    nhead = 8\n",
        "    num_layers = 3\n",
        "    dropout = 0.1\n",
        "\n",
        "    # 初始化模型\n",
        "    model = TransformerModel(\n",
        "        input_size=input_size,\n",
        "        d_model=d_model,\n",
        "        nhead=nhead,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    print_model_size(model)\n",
        "\n",
        "    # 优化器设置\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "    loss_function = nn.MSELoss(reduction='none')\n",
        "\n",
        "    # 训练循环\n",
        "    epochs = 20\n",
        "    best = float('-inf')\n",
        "    degraded = 0\n",
        "    best_model = model\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_mse, train_r2 = train_model(\n",
        "            model, train_loader, optimizer, loss_function, device\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "        val_mse, val_r2 = evaluate_model(model, val_loader)\n",
        "\n",
        "        print(f'epoch {epoch}:')\n",
        "        print(f'train loss {train_loss:.4f}, train_r2 {train_r2:.4f}, '\n",
        "              f'train_mse {train_mse:.4f}')\n",
        "        print(f'val_mse {val_mse:.4f}, val_r2 {val_r2:.4f}')\n",
        "        print(f'lr: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        if val_r2 > best:\n",
        "            best = val_r2\n",
        "            best_model = copy.deepcopy(model)\n",
        "            torch.save(best_model.state_dict(), f'./model/js_{run_type}_unnorm.pth')\n",
        "            degraded = 0\n",
        "        else:\n",
        "            degraded += 1\n",
        "\n",
        "        if degraded > 10:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model = best_model\n",
        "    test_mse, test_r2 = evaluate_model(model, test_loader)\n",
        "    print(f'test R2 score is {test_r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQyKZgiWJYXg"
      },
      "outputs": [],
      "source": [
        "lags_ : pl.DataFrame | None = None\n",
        "\n",
        "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
        "    \"\"\"Make predictions using either LSTM or Transformer model\"\"\"\n",
        "    global lags_\n",
        "    if lags is not None:\n",
        "        lags_ = lags\n",
        "\n",
        "    # 初始化预测DataFrame\n",
        "    predictions = test.select(\n",
        "        'row_id',\n",
        "        pl.lit(0.0).alias('responder_6'),\n",
        "    )\n",
        "\n",
        "    # 选择特征列\n",
        "    feature_cols = [f\"feature_{i:02d}\" for i in range(79)]\n",
        "\n",
        "    # 处理测试数据\n",
        "    test_features = test.select(feature_cols)\n",
        "    test_features = test_features.fill_null(strategy='forward').fill_null(0)\n",
        "\n",
        "    # 转换为tensor\n",
        "    X_test = test_features.to_numpy()\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 进行预测\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model(X_test_tensor).cpu().numpy()\n",
        "\n",
        "    # 更新预测结果\n",
        "    predictions = predictions.with_columns(pl.Series('responder_6', pred.ravel()))\n",
        "\n",
        "    # 验证输出格式\n",
        "    assert isinstance(predictions, (pl.DataFrame, pd.DataFrame))\n",
        "    assert list(predictions.columns) == ['row_id', 'responder_6']\n",
        "    assert len(predictions) == len(test)\n",
        "    print(predictions)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2v28mgbJYXg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "import kaggle_evaluation.jane_street_inference_server\n",
        "\n",
        "\n",
        "inference_server = kaggle_evaluation \\\n",
        "                        .jane_street_inference_server.JSInferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway(\n",
        "        (\n",
        "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
        "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
        "        )\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}